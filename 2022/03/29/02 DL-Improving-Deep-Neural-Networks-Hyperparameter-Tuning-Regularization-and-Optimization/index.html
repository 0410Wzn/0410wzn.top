<!DOCTYPE html><html class="hide-aside" lang="zh_Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization | 【W】</title><meta name="keywords" content="Deep_Learning"><meta name="author" content="[&quot;W&quot;]"><meta name="copyright" content="[&quot;W&quot;]"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization改善深层神经网络：超参数调试、正则化以及优化 Practical Aspects of Deep LearningTrain&#x2F;Dev&#x2F;Test sets训练神经网络时，我们需要做很多决策，例如：神经网络分为多少层、每层含有多少个">
<meta property="og:type" content="article">
<meta property="og:title" content="Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization">
<meta property="og:url" content="https://0410wzn.top/2022/03/29/02%20DL-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/index.html">
<meta property="og:site_name" content="【W】">
<meta property="og:description" content="Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization改善深层神经网络：超参数调试、正则化以及优化 Practical Aspects of Deep LearningTrain&#x2F;Dev&#x2F;Test sets训练神经网络时，我们需要做很多决策，例如：神经网络分为多少层、每层含有多少个">
<meta property="og:locale">
<meta property="og:image" content="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292221477.bmp">
<meta property="article:published_time" content="2022-03-29T14:17:42.000Z">
<meta property="article:modified_time" content="2022-03-29T14:23:54.103Z">
<meta property="article:author" content="W">
<meta property="article:tag" content="Deep_Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292221477.bmp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://0410wzn.top/2022/03/29/02%20DL-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-03-29 22:23:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="【W】" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/hexobackground/ev草・郎汎用02c1.bmp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292221477.bmp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">【W】</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-03-29T14:17:42.000Z" title="Created 2022-03-29 22:17:42">2022-03-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-03-29T14:23:54.103Z" title="Updated 2022-03-29 22:23:54">2022-03-29</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization"><a href="#Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization" class="headerlink" title="Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization"></a>Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</h1><p><strong>改善深层神经网络：超参数调试、正则化以及优化</strong></p>
<h2 id="Practical-Aspects-of-Deep-Learning"><a href="#Practical-Aspects-of-Deep-Learning" class="headerlink" title="Practical Aspects of Deep Learning"></a>Practical Aspects of Deep Learning</h2><h3 id="Train-Dev-Test-sets"><a href="#Train-Dev-Test-sets" class="headerlink" title="Train/Dev/Test sets"></a>Train/Dev/Test sets</h3><p>训练神经网络时，我们需要做很多决策，例如：神经网络分为多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数……</p>
<p>在创建新应用的过程中，我们不能从一开始就准确预测出这些信息和其它超参数，实际上，应用型机器学习是一个高度迭代的过程</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203252236335.png" alt="image-20220325223659264" style="zoom: 67%;">

<p>同时，创建高质量的训练数据集、验证集和测试集也有助于提高循环效率</p>
<p>需要注意的是，要确保验证集和测试集的数据来自于同一分布</p>
<hr>
<h3 id="Bias-Variance（偏差-方差）"><a href="#Bias-Variance（偏差-方差）" class="headerlink" title="Bias/Variance（偏差/方差）"></a>Bias/Variance（偏差/方差）</h3><p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203271954580.png" alt="image-20220327195357379"></p>
<p>理解偏差与方差的重要依据是，训练集误差和验证集误差</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Train set error</td>
<td>1%</td>
<td>15%</td>
<td>15%</td>
</tr>
<tr>
<td align="left">Dev set error</td>
<td>11%</td>
<td>16%</td>
<td>30%</td>
</tr>
<tr>
<td align="left"></td>
<td>high variance</td>
<td>high bias</td>
<td>high variance and  high bias</td>
</tr>
</tbody></table>
<p>最优误差，一般也被称为标准误差，其值接近于0%</p>
<p>通过查看训练集误差，可以判断数据的拟合情况可以判断是否有偏差(bias)问题，从而查看错误率的高低，当完成训练集训练，开始验证集验证时，可以来判断方差是否过高  </p>
<hr>
<h3 id="Basic-“recipe”-for-machine-learning"><a href="#Basic-“recipe”-for-machine-learning" class="headerlink" title="Basic “recipe” for machine learning"></a>Basic “recipe” for machine learning</h3><p>训练神经网络时用到的基本方法：</p>
<p>当初始模型训练完成后，首先判断算法的偏差高不高，如果偏差较高，就去评估训练集/测试集的性能，如果偏差确实比较高，甚至无法拟合训练集，就去选择一个新网络或是用更长的时间来训练网络</p>
<p>当偏差降低到可以接受的数值，先通过检查验证集性能检查一下方差是否无误，如果方差高，最好的解决办法就是采用更多数据，但当我们无法获得更多数据时，可以通过尝试正则化来减少过拟合</p>
<p>不断尝试，直到找到一个低偏差、低方差的框架</p>
<p>正则化是一种非常有效的减少方差的手段，正则化时会出现偏差方差权衡问题，偏差可能略有增加，如果网络足够大，增幅通常不会太高</p>
<hr>
<h3 id="Regularization（正则化）"><a href="#Regularization（正则化）" class="headerlink" title="Regularization（正则化）"></a>Regularization（正则化）</h3><p>当怀疑神经网络过度拟合了数据，即存在高偏差问题，一种方法是准备更多的数据，另一种方法就是正则化</p>
<p> 对于成本函数，$L2$正则化之后是这样的</p>
<p><font size="5px">$J(w,b),=,\frac{1}{m}*\sum_{i=1}^ml(\widehat{y}^{(i)},y^{[i]}),+,\frac{\lambda}{2m}||w||^2_2$</font></p>
<p><font size="5px">$||w||^2_2,=,\sum^{nx}_{j=1}w^2_j,=,w^Tw$（$L2$范数）</font></p>
<p>Q：为什么只需要正则化$w$，而不需要正则化$b$呢？</p>
<p>A：是可以加上的，不过因为$w$通常是一个高维参数矢量，已经可以表达高偏差问题，$w$有很多参数，我们不可      能拟合所有参数，而$b$只是单个数字，所以$w$几乎涵盖所有参数，而不是$b$，因为$b$只是众多参数中的一个</p>
<p>$L1$正则化则是：</p>
<p>$\frac{\lambda}{2m}\sum^{nx}_{i=1}||w||,=,\frac{\lambda}{2m}||w|_1$</p>
<p>$\lambda$是正则化参数，python中要用$lambd$来代替$lambda$，防止与保留字冲突</p>
<p><em><strong>如何在神经网络中实现正则化？</strong></em></p>
<p>神经网络含有一个成本函数</p>
<p><font size="5px">$J(w^{[1]},,b^{[1]},,…,,,w^{[l]},,b^{[l]}),=,\frac{1}{m}*\sum_{i=1}^ml(\widehat{y}^{(i)},y^{[i]}),+,\frac{\lambda}{2m}||w||^2_2$</font></p>
<p><font size="5px">$||w||^2_2,=,\sum^{n^{[l - 1]}}<em>{i=1}\sum^{n^{[l]}}</em>{j=1}(w^{[l]}_j)^2$</font></p>
<p>上述表达式是因为$w$是形状为$(n^{[l-1]},n^{[l]})$的矩阵，该矩阵范数被称为”Frobenius norm”（弗洛贝尼乌斯范数），它表示一个矩阵中所有元素的平方和</p>
<p>还如何使用该范数实现梯度下降呢？</p>
<p> 用backprop计算出$dw$的值，backprop会给出j对$w^{[l]}$的偏导数，将$w^{[l]}$替换为$w^{[l]},-,\alpha w^{[l]}$，就是之前我们额外增加的正则化项</p>
<p>再对上式做出如下改变 </p>
<p><font size="5px"> $dw^{[l]},=,(backprop),+,\frac{\lambda}{m}w^{[l]}$</font></p>
<p>替换后，$w^{[l]}$的定义就更新为了：</p>
<p><strong><font size="4.5px">$w^{[l]},=,w^{[l]},-,\alpha,[(backprop),+,\frac{\lambda}{m}w^{[l]}],=,w^{[l]},-,\frac{\alpha\lambda}{m}w^{[l]},-,\alpha(backprop)$</font></strong></p>
<p>在其中，矩阵$W$减去$\frac{\alpha\lambda}{m}$倍的它，即令矩阵$W$乘以系数$(1,-,\frac{\alpha\lambda}{m})$，该系数小于1，因此$L2$正则化也被称为“权重衰减”</p>
<hr>
<h3 id="Why-regularization-reduces-overfitting-为什么正则化可以减少方差问题"><a href="#Why-regularization-reduces-overfitting-为什么正则化可以减少方差问题" class="headerlink" title="Why regularization reduces overfitting (为什么正则化可以减少方差问题)"></a>Why regularization reduces overfitting (为什么正则化可以减少方差问题)</h3><p> 当$\lambda$被设置的足够大，权重矩阵$W$被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本消除了这些隐藏单元的许多影响   </p>
<hr>
<h3 id="Dropout-regularization（随机失活正则）"><a href="#Dropout-regularization（随机失活正则）" class="headerlink" title="Dropout regularization（随机失活正则）"></a>Dropout regularization（随机失活正则）</h3><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272142232.png" alt="image-20220327214208180" style="zoom:50%;">

<p>假设在训练上图所示的神经网络，它存在过拟合，这即是dropout要处理的</p>
<p>dropout会遍历网络的每一层，并设置消除神经网络中结点的概率</p>
<p>选择一些节点，删除一些节点的连线，得到一个更小的网络，然后用backprop方法进行训练</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272145627.png" alt="image-20220327214557566" style="zoom:50%;">

<p>那么，该<strong>如何实施dropout呢</strong>？</p>
<p>方法有几种，最常用的是<strong>inverted dropout（反向随机失活）</strong>，以下为例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是一个随机矩阵</span></span><br><span class="line"><span class="comment"># 这里的keep.prop是一个常数，表示保留某个隐藏单元的概率</span></span><br><span class="line"><span class="comment"># 假设让krrp.prop为0.8，则意味着消除任意一个隐藏单元的概率为0.2</span></span><br><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>], a3.shape[<span class="number">3</span>])  &lt;  keep.prop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从三层中获取激活函数</span></span><br><span class="line"><span class="comment"># 其作用即时过滤d3中所有等于0的元素</span></span><br><span class="line">a3 = np.multply(a3, d3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向外拓展a3</span></span><br><span class="line">a3 /= keep.prop</span><br></pre></td></tr></table></figure>

<p>需要注意的是，我们在测试阶段不使用dropout函数，因为在测试阶段进行预测时，我们不期望输出结果是随机的如果测试阶段应用dropout函数，预测会受到干扰，即确保在测试阶段不执行dropout来调整数值范围，激活函数的预期结果也不会发生变化。</p>
<hr>
<h3 id="Understanding-dropout"><a href="#Understanding-dropout" class="headerlink" title="Understanding dropout"></a>Understanding dropout</h3><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272221208.png" alt="image-20220327222129134" style="zoom:67%;">

<p>如果担心某些层比其他层更容易发生过拟合，可以把某些层的keep_prob值设置的比其他层更低，确定是为了使用交叉验证，需要搜集更多的超级参数；另一种方案是在一些层上应用dropout，而一些层则不用，应用dropout的层只含有一个超级参数，即keep_prob   </p>
<p>但请牢记，dropout是一种正则化方法，它有助于预防过拟合，因此，除非算法过拟合，一般是不会用到的</p>
<hr>
<h3 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h3><p>除了上述的两种方法，还有几种方法可以减少神经网络中的过拟合</p>
<p>一    数据扩增</p>
<p>当数据不足时，对于图片，可以翻转/随意裁剪来扩大数据集，额外生成假训练数据；对于文字，可以随意旋转/扭曲数字来增加数据</p>
<p>因此，数据扩增可以作为正则化方法使用，实际功能也与其类似</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272238262.png" alt="image-20220327223830175" style="zoom: 50%;">

<p>二    early stopping</p>
<p>运行梯度下降时，我们可以绘制训练误差，或只绘制代价函数$J$的优化过程，在训练集上用0-1记录分类误差次数呈下降趋势</p>
<p>除此之外，我们还可以绘制验证集误差</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272239599.png" alt="image-20220327223936537" style="zoom:50%;">

<p>可以发现，验证集误差通常先呈下降趋势，在某个节点处重新回升</p>
<p>当还未在神经网络上运行太多迭代过程时，参数$w$接近于0，因为在随机初始化$w$时，它的值可能是较小的随机值，而在迭代和训练过程中，$w$的值会变得越来越大，early stopping即是，在中间点停止迭代过程，我们得到一个$w$值中等大小的弗洛贝尼乌斯范数</p>
<p>其优点是，只运行一次坡度下降，找出$w$的值，而无需尝试$L2$正则化超级参数$\lambda$的很多值</p>
<hr>
<h3 id="Normalizing-inputs（归一化输入）"><a href="#Normalizing-inputs（归一化输入）" class="headerlink" title="Normalizing inputs（归一化输入）"></a>Normalizing inputs（归一化输入）</h3><p>当训练神经网络时，其中一个加速训练的方法就是归一化输入</p>
<p><strong>归一化输入需要两个步骤：</strong></p>
<p>第一是零均值化</p>
<p><font size="5px">$\mu,=,\frac{1}{m}\sum^m_{i=1}x^{[i]}$；$x,:=,x,-,\mu$</font></p>
<p>上面式子的意思是：移动训练集，直到它完成零均值化</p>
<p>如下图</p>
<p> <img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272056208.png" alt="image-20220327205611102" style="zoom:50%;"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203272056858.png" alt="image-20220327205653755" style="zoom:50%;"></p>
<p>第二步是归一化方差</p>
<p>上图中的特征$X1$方差比特征$X2$的方差要大得多</p>
<p>我们要做的：</p>
<p><font size="5px">$\sigma^2,=,\frac{1}{m}\sum^m_{i=1}x^{[i]}**2$</font></p>
<p>******是节点$y$的平方，$\sigma^2$是一个向量，它的每个特征都有方差</p>
<p><font size="5px">$x ,/=\sigma^2$</font></p>
 <img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281130078.png" alt="image-20220328113020031" style="zoom:50%;">

<p><strong>为什么要归一化输入？</strong></p>
<p>对于代价函数，如果我们使用非归一化输入，函数就会如下图一般，呈细长狭窄状</p>
<p> <img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281134210.png" alt="image-20220328113429162" style="zoom:67%;"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281136349.png" style="zoom:80%;"></p>
<p> 而归一化之后</p>
<p> <img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281137994.png" alt="image-20220328113718942" style="zoom:67%;"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281137912.png" alt="image-20220328113750855" style="zoom:67%;"></p>
<p>不归一化时，由于在空间上分布也不是平滑的，使得梯度下降法需要小步长、多次迭代才可能实现目的，而归一化后，梯度下降法能够更直接的使用较大步长查找最小值，这使得代价函数优化起来更简单也更快速</p>
<hr>
<h3 id="Vanishing-exploding-gradients（梯度-消失-爆炸-）"><a href="#Vanishing-exploding-gradients（梯度-消失-爆炸-）" class="headerlink" title="Vanishing/exploding gradients（梯度 消失/爆炸 ）"></a>Vanishing/exploding gradients（梯度 消失/爆炸 ）</h3><p>当训练一个极深的神经网络时</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281202018.png" alt="image-20220328120206908"></p>
<p>如果权重$w$只比1略大一点，或者说只比单位矩阵大一点，神经网络的激活函数将爆炸式增长</p>
<p>如果权重$w$只比1略小一点，在神经网络中，激活函数将以指数级递减</p>
<p>一个防止办法就是 — 权重初始化</p>
<hr>
<h3 id="Weight-initialization-for-deep-networks（神经网络的权重初始化）"><a href="#Weight-initialization-for-deep-networks（神经网络的权重初始化）" class="headerlink" title="Weight initialization for deep networks（神经网络的权重初始化）"></a>Weight initialization for deep networks（神经网络的权重初始化）</h3><p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281349762.png" alt="image-20220328134918716"></p>
<p>对于这样一个简易的神经元</p>
<p>$Z,=,w_1x_1,+w_2x_2,+,…,+,w_nx_n$（暂且省略$b$）</p>
<p>我们可以发现，$n$越大，我们希望的$w$越小</p>
<p>最合理的方法就是，设置$w_i,=,\frac{1}{n}$ （$n$表示神经元的输入特征数量）</p>
<p>实际上要做的就是，设置某层的权重矩阵$w^{[l]},=,np.random.randn(shape),*,np.sqrt(\frac{1}{n^{[l-1]}})$</p>
<p> 对于tanh函数，可以使用下图第一个公式</p>
 <img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281400519.png" alt="image-20220328140054458" style="zoom:50%;">

<hr>
<h3 id="Numerical-approximation-of-gradients（梯度的数值逼近）"><a href="#Numerical-approximation-of-gradients（梯度的数值逼近）" class="headerlink" title="Numerical approximation of gradients（梯度的数值逼近）"></a>Numerical approximation of gradients（梯度的数值逼近）</h3><p>在执行backprop时，有一个步骤叫做梯度检验，它的作用是确保backprop正确实施，为了逐渐实现梯度检验，首先要了解如何对梯度做数值逼近</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281410198.png" alt="image-20220328141035130"></p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281412315.png" alt="image-20220328141256250"></p>
<p>在梯度检验时，我们使用双边误差，即$\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$，而不使用单边公差，因为它不够准确</p>
<hr>
<h3 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h3><p>假设网络中有以下参数<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281418497.png" alt="image-20220328141856456" style="zoom: 50%;">，为了执行梯度检验，首先要做的就是把所有参数转换成一个巨大的向量数据$\theta$</p>
<p>即$J(w^{[1]},,b^{[1]},,…,,,w^{[l]},,b^{[l]}),=,J(\theta)$</p>
<p>接着，得到与$w$和$b$顺序相同的数据<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281422228.png" alt="image-20220328142235180" style="zoom: 50%;">，用他们来初始化巨大向量$d\theta$，它与$\theta$具有相同维度</p>
<p>下面的问题是$d\theta$与代价函数$J$的梯度或坡度有何关系，下面即是检验的过程</p>
<p>首先要明确$J$是超级参数$\theta$的一个函数</p>
<p>为了实施梯度检验，需要做的就是循环执行，从而对每个i，也就是对每个$\theta$组成元素，计算$d\theta ,approx[i]$的值</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281433467.png" alt="image-20220328143305406"></p>
<p>由上节课的的知识我们知道</p>
<p>上述式子的值</p>
<p><font size="5px">$\approx,d\theta^{[i]},=,\frac{\delta J}{\delta \theta}$</font></p>
<p>$d\theta^{[i]}$是代价函数的偏导数</p>
<p>经过对所有值进行此运算，最后得到两个向量，其中之一即使$d\theta$的逼近值$d\theta approx$，它与$d\theta$具有相同维度，我们要做的就是验证这些向量是否彼此接近</p>
<p>如何定义两个向量互相接近？</p>
<p>计算两个向量距离$d\theta approx-d\theta$的欧几里得范数$||d\theta approx-d\theta||_2$（误差平方之和），然后求平方根，得到欧式距离，之后用向量长度做归一化，结果为</p>
<p><font size="5px">$\frac{||d\theta approx-d\theta||_2}{||d\theta approx||_2 + ||d\theta||_2}$</font></p>
<p>（分母只是用来预防这些向量太小或太大，分母使得方程式变为比率）</p>
<p>如果得到的$\varepsilon$是$10^{-7}$，或更小一点，这意味着导数逼近很可能是正确的，</p>
<p>但如果其范围在$10^{-5}$范围内，需要注意一下，确保一下没有一项误差过大，如果有，那么这里可能存在bug</p>
<p>值为$10^{-3}$或比它更大，应该非常担心是否存在bug</p>
<hr>
<h3 id="Gradient-Checking-implementation-notes"><a href="#Gradient-Checking-implementation-notes" class="headerlink" title="Gradient Checking implementation notes"></a>Gradient Checking implementation notes</h3><ul>
<li><p>不要在训练中使用梯度检验，它只用于调试</p>
</li>
<li><p>如果算法的梯度检验失败，要检查所有项，试着找出bug</p>
</li>
<li><p>在实施梯度检验时，，如果使用正则化，请注意正则项，当调试时，将dropout关上再进行调试</p>
</li>
</ul>
<hr>
<h2 id="Optimization-Algorithms（优化算法）"><a href="#Optimization-Algorithms（优化算法）" class="headerlink" title="Optimization Algorithms（优化算法）"></a>Optimization Algorithms（优化算法）</h2><h3 id="Mini-batch-gradient-descent（Mini-batch梯度下降法）"><a href="#Mini-batch-gradient-descent（Mini-batch梯度下降法）" class="headerlink" title="Mini-batch gradient descent（Mini-batch梯度下降法）"></a>Mini-batch gradient descent（Mini-batch梯度下降法）</h3><p>向量化可以有效的对m个例子进行计算，允许在处理整个训练集同时，无需明确某个明确的公式，因此我们要把数据放到巨大的矩阵$X$中去</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281615730.png" alt="image-20220328161513667" style="zoom: 67%;">

<p>尽管向量化可以使我们较为轻松的处理大量数据的样本，但如果m很大的话，处理数度依旧会变缓慢</p>
<p>在对整个训练集执行梯度下降法时，必须处理整个训练集，然后才能执行一步梯度下降法，然后需要重新处理数据，才能进行下一步梯度下降法</p>
<p>如果在处理完所有数据之前，先让梯度下降法处理一部分，相信算法速度会更快</p>
<p>将训练集分割为小一点的子训练集，这些子集就被取名为”Mini-batch”，显示格式即为$X^$与$Y^$</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281623911.png" alt="image-20220328162330843"></p>
<p>（这里回顾一下括号，小括号上标指第几个元素，中括号指神经网络的层数）</p>
<p>与原本同时处理整个batch训练集样本不同，Mini-batch梯度下降每次同时处理的是单个的mini-batch $X^$与$Y^$，而不是同时处理全部的$X$和$Y$训练集</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281656226.png"></p>
<p>通过上图可以发现，实现的基本步骤与与原本执行梯度下降法别无二致，区别就在于对象变为了 $X^$与$Y^$，</p>
<hr>
<h3 id="Understanding-mini-batch-gradient-descent"><a href="#Understanding-mini-batch-gradient-descent" class="headerlink" title="Understanding mini-batch gradient descent"></a>Understanding mini-batch gradient descent</h3><p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281938061.png" alt="image-20220328193836899"></p>
<p>当m=1时，就变为了“随机梯度下降法”，它的每一项都是一个mini-batch。</p>
<p>而“随机梯度下降法”在寻找最小值时，大部分时候都是朝着全局最小值前进，但有些时候也会因为单个样本方向不对从而远离最小值，因此随机梯度下降法是有很多噪声的，平均来看，它最终会接近最小值，不过有时候也会出现方向错误。</p>
<p>因为随机梯度下降法永远不会收敛，而是会一直在在最小值附近活动，但并不会达到最小值并停留于此</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203281947124.png" alt="image-20220328194715017" style="zoom:50%;">

<p>随机梯度下降法的优点是，在处理单个数据是表现优异；但其缺点是，会失去所有向量化带来的加速，因为一次性只处理了一个训练样本，这样效率过于低下</p>
<p>因此，选择不大不小的mini-batch尺寸，会发现两个好处，一方面，得到了大量向量化，加快了多次处理样本速度；另一方面，无需等待整个训练集被·处理完  ，就可以进行后续工作</p>
<hr>
<h3 id="Exponentially-weighted-averages（指数加权平均）"><a href="#Exponentially-weighted-averages（指数加权平均）" class="headerlink" title="Exponentially weighted averages（指数加权平均）"></a>Exponentially weighted averages（指数加权平均）</h3><p>再去了解一些其他算法之前，首先要了解指数加权平均</p>
<p>以温度为例</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203282009005.png" alt="image-20220328200940903"></p>
<p>如果我们要计算趋势，也就是温度的局部平均值/移动平均值，要做的是</p>
<p>$v_0,=,0$；</p>
<p>$v_1,=,0.9*v_0,+,0.1\theta_1$；</p>
<p>$v_2,=,0.9*v_1,+,0.1\theta_2$.</p>
<p>…..；</p>
<p>$v_t,=,0.9*v_{t-1},+,0.1\theta_t$</p>
<p>如此计算，再用红线作图，得到如下结果</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203282014672.png" alt="image-20220328201420582" style="zoom: 67%;">

<p>我们得到了移动平均值，每日温度的指数加权平均值</p>
<p>改写一下式子</p>
<p>$v_t,=,\beta*v_{t-1},+,(1-\beta)\theta_t$</p>
<p>同时在计算时，可将$v_t$视为大概是$\frac{1}{(1-\beta)}$的每日温度，如果$\beta$为0.9，就是上面的红线</p>
<p>我们取一个稍大一点的值 — 0.98，这即是平均了一下过去50天的温度，作图后得到以下绿线</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203282020970.png" alt="image-20220328202031854" style="zoom:67%;">

<p>当我们取高值的时候，会发现曲线平缓了一些，那是因为多平均了几天的温度，因此这个曲线波动更小、更加平坦，缺点是曲线进一步右移，平均了更多数据，指数加权公式，在温度变化时，适应地更缓慢一些</p>
<p>我们再取一个较小的值 — 0.5，由于数据少，波动也更为明显，如下图黄线所示</p>
<img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203282026596.png" alt="image-20220328202637480" style="zoom:67%;">

<p>因此，我们可以通过调整指数加权平均数来寻找适合的“中间的某个值”，就像上图的红线一样</p>
<hr>
<h3 id="Understanding-exponentially-weighted-averages"><a href="#Understanding-exponentially-weighted-averages" class="headerlink" title="Understanding exponentially weighted averages"></a>Understanding exponentially weighted averages</h3><p>沿用上一题得到的公式</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203282032304.png" alt="image-20220328203204105"></p>
<p>可以知道$V_{100}$即是选取的每日温度与指数衰减函数相乘，然后求和</p>
<hr>
<h3 id="Bais-correction-in-exponentially-weighted-averages"><a href="#Bais-correction-in-exponentially-weighted-averages" class="headerlink" title="Bais correction in exponentially weighted averages"></a>Bais correction in exponentially weighted averages</h3><p>Bais correction — 偏差修正  </p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203292148461.png" alt="image-20220329214802325"></p>
<hr>
<h3 id="Gradient-descent-with-momentum（momentum梯度下降法）"><a href="#Gradient-descent-with-momentum（momentum梯度下降法）" class="headerlink" title="Gradient descent with momentum（momentum梯度下降法）"></a>Gradient descent with momentum（momentum梯度下降法）</h3><p>基本思想：计算梯度的指数加权平均数，并利用该梯度更新权重</p>
<p><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/%E5%A4%8D%E4%B9%A0/202203292156527.png" alt="image-20220329215612483"></p>
<p>上图是batch梯度下降法的实例，可以看出，在寻找最小梯度时经历了不少上下波动，而这种频繁的上下波动也减缓了梯度下降法的速度</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">W</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://0410wzn.top/2022/03/29/02%20DL-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/">https://0410wzn.top/2022/03/29/02%20DL-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep_Learning</a></div><div class="post_share"><div class="social-share" data-image="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292221477.bmp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/03/29/01%20DL-Neural-Networks-and-Deep-Learning/"><img class="next-cover" src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292222632.bmp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Neural_Networks_and_Deep_Learning</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/03/29/00%20DL-Aim/" title="DL_Aim"><img class="cover" src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292219385.bmp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-29</div><div class="title">DL_Aim</div></div></a></div><div><a href="/2022/03/29/01%20DL-Neural-Networks-and-Deep-Learning/" title="Neural_Networks_and_Deep_Learning"><img class="cover" src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292222632.bmp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-29</div><div class="title">Neural_Networks_and_Deep_Learning</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/hexobackground/ev草・郎汎用02c1.bmp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">W</div><div class="author-info__description">Less intrests , more interest .</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization"><span class="toc-number">1.</span> <span class="toc-text">Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Practical-Aspects-of-Deep-Learning"><span class="toc-number">1.1.</span> <span class="toc-text">Practical Aspects of Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-Dev-Test-sets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Train&#x2F;Dev&#x2F;Test sets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias-Variance%EF%BC%88%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%EF%BC%89"><span class="toc-number">1.1.2.</span> <span class="toc-text">Bias&#x2F;Variance（偏差&#x2F;方差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-%E2%80%9Crecipe%E2%80%9D-for-machine-learning"><span class="toc-number">1.1.3.</span> <span class="toc-text">Basic “recipe” for machine learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularization%EF%BC%88%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%89"><span class="toc-number">1.1.4.</span> <span class="toc-text">Regularization（正则化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-regularization-reduces-overfitting-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E6%96%B9%E5%B7%AE%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.5.</span> <span class="toc-text">Why regularization reduces overfitting (为什么正则化可以减少方差问题)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-regularization%EF%BC%88%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB%E6%AD%A3%E5%88%99%EF%BC%89"><span class="toc-number">1.1.6.</span> <span class="toc-text">Dropout regularization（随机失活正则）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Understanding-dropout"><span class="toc-number">1.1.7.</span> <span class="toc-text">Understanding dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other-regularization-methods"><span class="toc-number">1.1.8.</span> <span class="toc-text">Other regularization methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalizing-inputs%EF%BC%88%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%89"><span class="toc-number">1.1.9.</span> <span class="toc-text">Normalizing inputs（归一化输入）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Vanishing-exploding-gradients%EF%BC%88%E6%A2%AF%E5%BA%A6-%E6%B6%88%E5%A4%B1-%E7%88%86%E7%82%B8-%EF%BC%89"><span class="toc-number">1.1.10.</span> <span class="toc-text">Vanishing&#x2F;exploding gradients（梯度 消失&#x2F;爆炸 ）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Weight-initialization-for-deep-networks%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%89"><span class="toc-number">1.1.11.</span> <span class="toc-text">Weight initialization for deep networks（神经网络的权重初始化）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Numerical-approximation-of-gradients%EF%BC%88%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91%EF%BC%89"><span class="toc-number">1.1.12.</span> <span class="toc-text">Numerical approximation of gradients（梯度的数值逼近）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Checking"><span class="toc-number">1.1.13.</span> <span class="toc-text">Gradient Checking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Checking-implementation-notes"><span class="toc-number">1.1.14.</span> <span class="toc-text">Gradient Checking implementation notes</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimization-Algorithms%EF%BC%88%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">Optimization Algorithms（优化算法）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch-gradient-descent%EF%BC%88Mini-batch%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">Mini-batch gradient descent（Mini-batch梯度下降法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Understanding-mini-batch-gradient-descent"><span class="toc-number">1.2.2.</span> <span class="toc-text">Understanding mini-batch gradient descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exponentially-weighted-averages%EF%BC%88%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">Exponentially weighted averages（指数加权平均）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Understanding-exponentially-weighted-averages"><span class="toc-number">1.2.4.</span> <span class="toc-text">Understanding exponentially weighted averages</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bais-correction-in-exponentially-weighted-averages"><span class="toc-number">1.2.5.</span> <span class="toc-text">Bais correction in exponentially weighted averages</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-descent-with-momentum%EF%BC%88momentum%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%89"><span class="toc-number">1.2.6.</span> <span class="toc-text">Gradient descent with momentum（momentum梯度下降法）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/03/29/02%20DL-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/" title="Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292221477.bmp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization"/></a><div class="content"><a class="title" href="/2022/03/29/02%20DL-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/" title="Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization">Improving_Deep_Neural_Networks_Hyperparameter_Tuning,Regularization_and_Optimization</a><time datetime="2022-03-29T14:17:42.000Z" title="Created 2022-03-29 22:17:42">2022-03-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/29/01%20DL-Neural-Networks-and-Deep-Learning/" title="Neural_Networks_and_Deep_Learning"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292222632.bmp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Neural_Networks_and_Deep_Learning"/></a><div class="content"><a class="title" href="/2022/03/29/01%20DL-Neural-Networks-and-Deep-Learning/" title="Neural_Networks_and_Deep_Learning">Neural_Networks_and_Deep_Learning</a><time datetime="2022-03-29T14:15:26.000Z" title="Created 2022-03-29 22:15:26">2022-03-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/29/00%20DL-Aim/" title="DL_Aim"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292219385.bmp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DL_Aim"/></a><div class="content"><a class="title" href="/2022/03/29/00%20DL-Aim/" title="DL_Aim">DL_Aim</a><time datetime="2022-03-29T14:12:57.000Z" title="Created 2022-03-29 22:12:57">2022-03-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/02/React%EF%BC%88%E4%B8%80%EF%BC%89/" title="React（一）"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/CTF/bg03旧校舎03(秋)b.bmp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="React（一）"/></a><div class="content"><a class="title" href="/2022/02/02/React%EF%BC%88%E4%B8%80%EF%BC%89/" title="React（一）">React（一）</a><time datetime="2022-02-02T13:54:07.000Z" title="Created 2022-02-02 21:54:07">2022-02-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/18/C++STL%E6%80%BB%E7%BB%93/" title="C++STL总结"><img src="https://wzn0410.oss-cn-beijing.aliyuncs.com/img/CTF/im01オープニング02c_ピン前a.bmp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++STL总结"/></a><div class="content"><a class="title" href="/2022/01/18/C++STL%E6%80%BB%E7%BB%93/" title="C++STL总结">C++STL总结</a><time datetime="2022-01-18T11:54:20.000Z" title="Created 2022-01-18 19:54:20">2022-01-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://wzn0410.oss-cn-beijing.aliyuncs.com/img/复习/202203292221477.bmp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By W</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="000PeZCQ1i4XVs" data-server="tencent" data-type="artist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><script async data-pjax src="/js/background.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>